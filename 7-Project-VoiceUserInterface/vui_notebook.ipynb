{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Voice User Interfaces\n",
    "\n",
    "## Project: Speech Recognition with Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the Jupyter Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this Jupyter notebook.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction  \n",
    "\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end automatic speech recognition (ASR) pipeline!  Your completed pipeline will accept raw audio as input and return a predicted transcription of the spoken language.  The full pipeline is summarized in the figure below.\n",
    "\n",
    "<img src=\"images/pipeline.png\">\n",
    "\n",
    "- **STEP 1** is a pre-processing step that converts raw audio to one of two feature representations that are commonly used for ASR. \n",
    "- **STEP 2** is an acoustic model which accepts audio features as input and returns a probability distribution over all potential transcriptions.  After learning about the basic types of neural networks that are often used for acoustic modeling, you will engage in your own investigations, to design your own acoustic model!\n",
    "- **STEP 3** in the pipeline takes the output from the acoustic model and returns a predicted transcription.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [The Data](#thedata)\n",
    "- [**STEP 1**](#step1): Acoustic Features for Speech Recognition\n",
    "- [**STEP 2**](#step2): Deep Neural Networks for Acoustic Modeling\n",
    "    - [Model 0](#model0): RNN\n",
    "    - [Model 1](#model1): RNN + TimeDistributed Dense\n",
    "    - [Model 2](#model2): CNN + RNN + TimeDistributed Dense\n",
    "    - [Model 3](#model3): Deeper RNN + TimeDistributed Dense\n",
    "    - [Model 4](#model4): Bidirectional RNN + TimeDistributed Dense\n",
    "    - [Models 5+](#model5)\n",
    "    - [Compare the Models](#compare)\n",
    "    - [Final Model](#final)\n",
    "- [**STEP 3**](#step3): Obtain Predictions\n",
    "\n",
    "<a id='thedata'></a>\n",
    "## The Data\n",
    "\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  [LibriSpeech](http://www.danielpovey.com/files/2015_icassp_librispeech.pdf) is a large corpus of English-read speech, designed for training and evaluating models for ASR.  The dataset contains 1000 hours of speech derived from audiobooks.  We will work with a small subset in this project, since larger-scale data would take a long while to train.  However, after completing this project, if you are interested in exploring further, you are encouraged to work with more of the data that is provided [online](http://www.openslr.org/12/).\n",
    "\n",
    "In the code cells below, you will use the `vis_train_features` module to visualize a training example.  The supplied argument `index=0` tells the module to extract the first example in the training set.  (You are welcome to change `index=0` to point to a different training example, if you like, but please **DO NOT** amend any other code in the cell.)  The returned variables are:\n",
    "- `vis_text` - transcribed text (label) for the training example.\n",
    "- `vis_raw_audio` - raw audio waveform for the training example.\n",
    "- `vis_mfcc_feature` - mel-frequency cepstral coefficients (MFCCs) for the training example.\n",
    "- `vis_spectrogram_feature` - spectrogram for the training example. \n",
    "- `vis_audio_path` - the file path to the training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data\n",
    "\n",
    "* wget http://www.openslr.org/resources/12/dev-clean.tar.gz\n",
    "\n",
    "* tar -xzvf dev-clean.tar.gz\n",
    "* wget http://www.openslr.org/resources/12/test-clean.tar.gz\n",
    "\n",
    "* tar -xzvf test-clean.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting .flac to .wav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "script was found in [covert flac to wav](https://unix.stackexchange.com/questions/341436/a-script-to-convert-flac-files-to-wav-is-not-working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# for root, dirs, files in os.walk(\"../../Data/speech2text_data/data/LibriSpeech/test-clean/\"):\n",
    "#     if len(root.split(\"/\")) == 9:\n",
    "#         root = root + '/'\n",
    "#         print(root)\n",
    "#         subprocess.call(['./flac_to_wav.sh', root]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "\n",
    "# for root, dirs, files in os.walk(\"../../Data/speech2text_data/data/LibriSpeech/dev-clean/\"):\n",
    "#     if len(root.split(\"/\")) == 9:\n",
    "#         root = root + '/'\n",
    "#         print(root)\n",
    "#         subprocess.call(['./flac_to_wav.sh', root]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### updating the json files provided in the root.\n",
    "i decided it is easier to modify the existing ones than to use the provided tool in the class to generate one from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# DATA_FILENAME = \"./valid_corpus_updated.json\"\n",
    "# files_new_location_list = \"../../Data/speech2text_data/data/LibriSpeech/test-clean\".split(\"/\")\n",
    "# with open(DATA_FILENAME, mode='w', encoding='utf-8') as feedsjson:\n",
    "#     with open(\"./valid_corpus.json\") as json_file:\n",
    "#         for line in json_file:\n",
    "#             validation_json = json.loads(line)\n",
    "#             file_old_location_list = validation_json[\"key\"].split(\"/\")\n",
    "#             file_old_location_list[:5] = files_new_location_list\n",
    "#             file_new_location =  \"/\".join(file_old_location_list)\n",
    "#             validation_json[\"key\"] = file_new_location\n",
    "#             json.dump(validation_json, feedsjson)\n",
    "#             feedsjson.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# DATA_FILENAME = \"./train_corpus_updated.json\"\n",
    "# files_new_location_list = \"../../Data/speech2text_data/data/LibriSpeech/dev-clean\".split(\"/\")\n",
    "# with open(DATA_FILENAME, mode='w', encoding='utf-8') as feedsjson:\n",
    "#     with open(\"./train_corpus.json\") as json_file:\n",
    "#         for line in json_file:\n",
    "#             validation_json = json.loads(line)\n",
    "#             file_old_location_list = validation_json[\"key\"].split(\"/\")\n",
    "#             file_old_location_list[:5] = files_new_location_list\n",
    "#             file_new_location =  \"/\".join(file_old_location_list)\n",
    "#             validation_json[\"key\"] = file_new_location\n",
    "#             json.dump(validation_json, feedsjson)\n",
    "#             feedsjson.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from data_generator import vis_train_features\n",
    "\n",
    "# extract label and audio features for a single training example\n",
    "vis_text, vis_raw_audio, vis_mfcc_feature, vis_spectrogram_feature, vis_audio_path = vis_train_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell visualizes the audio waveform for your chosen example, along with the corresponding transcript.  You also have the option to play the audio in the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from data_generator import vis_train_features, plot_raw_audio\n",
    "from IPython.display import Audio\n",
    "%matplotlib inline\n",
    "\n",
    "# plot audio signal\n",
    "plot_raw_audio(vis_raw_audio)\n",
    "# print length of audio signal\n",
    "display(Markdown('**Shape of Audio Signal** : ' + str(vis_raw_audio.shape)))\n",
    "# print transcript corresponding to audio clip\n",
    "display(Markdown('**Transcript** : ' + str(vis_text)))\n",
    "# play the audio file\n",
    "Audio(vis_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## STEP 1: Acoustic Features for Speech Recognition\n",
    "\n",
    "For this project, you won't use the raw audio waveform as input to your model.  Instead, we provide code that first performs a pre-processing step to convert the raw audio to a feature representation that has historically proven successful for ASR models.  Your acoustic model will accept the feature representation as input.\n",
    "\n",
    "In this project, you will explore two possible feature representations.  _After completing the project_, if you'd like to read more about deep learning architectures that can accept raw audio input, you are encouraged to explore this [research paper](https://pdfs.semanticscholar.org/a566/cd4a8623d661a4931814d9dffc72ecbf63c4.pdf).\n",
    "\n",
    "### Spectrograms\n",
    "\n",
    "The first option for an audio feature representation is the [spectrogram](https://www.youtube.com/watch?v=_FatxGN3vAM).  In order to complete this project, you will **not** need to dig deeply into the details of how a spectrogram is calculated; but, if you are curious, the code for calculating the spectrogram was borrowed from [this repository](https://github.com/baidu-research/ba-dls-deepspeech).  The implementation appears in the `utils.py` file in your repository.\n",
    "\n",
    "The code that we give you returns the spectrogram as a 2D tensor, where the first (_vertical_) dimension indexes time, and the second (_horizontal_) dimension indexes frequency.  To speed the convergence of your algorithm, we have also normalized the spectrogram.  (You can see this quickly in the visualization below by noting that the mean value hovers around zero, and most entries in the tensor assume values close to zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import plot_spectrogram_feature\n",
    "\n",
    "# plot normalized spectrogram\n",
    "plot_spectrogram_feature(vis_spectrogram_feature)\n",
    "# print shape of spectrogram\n",
    "display(Markdown('**Shape of Spectrogram** : ' + str(vis_spectrogram_feature.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-Frequency Cepstral Coefficients (MFCCs)\n",
    "\n",
    "The second option for an audio feature representation is [MFCCs](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum).  You do **not** need to dig deeply into the details of how MFCCs are calculated, but if you would like more information, you are welcome to peruse the [documentation](https://github.com/jameslyons/python_speech_features) of the `python_speech_features` Python package.  Just as with the spectrogram features, the MFCCs are normalized in the supplied code.\n",
    "\n",
    "The main idea behind MFCC features is the same as spectrogram features: at each time window, the MFCC feature yields a feature vector that characterizes the sound within the window.  Note that the MFCC feature is much lower-dimensional than the spectrogram feature, which could help an acoustic model to avoid overfitting to the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import plot_mfcc_feature\n",
    "\n",
    "# plot normalized MFCC\n",
    "plot_mfcc_feature(vis_mfcc_feature)\n",
    "# print shape of MFCC\n",
    "display(Markdown('**Shape of MFCC** : ' + str(vis_mfcc_feature.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you construct your pipeline, you will be able to choose to use either spectrogram or MFCC features.  If you would like to see different implementations that make use of MFCCs and/or spectrograms, please check out the links below:\n",
    "- This [repository](https://github.com/baidu-research/ba-dls-deepspeech) uses spectrograms.\n",
    "- This [repository](https://github.com/mozilla/DeepSpeech) uses MFCCs.\n",
    "- This [repository](https://github.com/buriburisuri/speech-to-text-wavenet) also uses MFCCs.\n",
    "- This [repository](https://github.com/pannous/tensorflow-speech-recognition/blob/master/speech_data.py) experiments with raw audio, spectrograms, and MFCCs as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## STEP 2: Deep Neural Networks for Acoustic Modeling\n",
    "\n",
    "In this section, you will experiment with various neural network architectures for acoustic modeling.  \n",
    "\n",
    "You will begin by training five relatively simple architectures.  **Model 0** is provided for you.  You will write code to implement **Models 1**, **2**, **3**, and **4**.  If you would like to experiment further, you are welcome to create and train more models under the **Models 5+** heading.  \n",
    "\n",
    "All models will be specified in the `sample_models.py` file.  After importing the `sample_models` module, you will train your architectures in the notebook.\n",
    "\n",
    "After experimenting with the five simple architectures, you will have the opportunity to compare their performance.  Based on your findings, you will construct a deeper architecture that is designed to outperform all of the shallow models.\n",
    "\n",
    "For your convenience, we have designed the notebook so that each model can be specified and trained on separate occasions.  That is, say you decide to take a break from the notebook after training **Model 1**.  Then, you need not re-execute all prior code cells in the notebook before training **Model 2**.  You need only re-execute the code cell below, that is marked with **`RUN THIS CODE CELL IF YOU ARE RESUMING THE NOTEBOOK AFTER A BREAK`**, before transitioning to the code cells corresponding to **Model 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# RUN THIS CODE CELL IF YOU ARE RESUMING THE NOTEBOOK AFTER A BREAK #\n",
    "#####################################################################\n",
    "\n",
    "# allocate 50% of GPU memory (if you like, feel free to change this)\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf \n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 1.0\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "# watch for any changes in the sample_models module, and reload it automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# import NN architectures for speech recognition\n",
    "from sample_models import *\n",
    "# import function for training acoustic model\n",
    "from train_utils import train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model0'></a>\n",
    "### Model 0: RNN\n",
    "\n",
    "Given their effectiveness in modeling sequential data, the first acoustic model you will use is an RNN.  As shown in the figure below, the RNN we supply to you will take the time sequence of audio features as input.\n",
    "\n",
    "<img src=\"images/simple_rnn.png\" width=\"50%\">\n",
    "\n",
    "At each time step, the speaker pronounces one of 28 possible characters, including each of the 26 letters in the English alphabet, along with a space character (\" \"), and an apostrophe (').\n",
    "\n",
    "The output of the RNN at each time step is a vector of probabilities with 29 entries, where the $i$-th entry encodes the probability that the $i$-th character is spoken in the time sequence.  (The extra 29th character is an empty \"character\" used to pad training examples within batches containing uneven lengths.)  If you would like to peek under the hood at how characters are mapped to indices in the probability vector, look at the `char_map.py` file in the repository.  The figure below shows an equivalent, rolled depiction of the RNN that shows the output layer in greater detail. \n",
    "\n",
    "<img src=\"images/simple_rnn_unrolled.png\" width=\"60%\">\n",
    "\n",
    "The model has already been specified for you in Keras.  To import it, you need only run the code cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = simple_rnn_model(input_dim=161) # change to 13 if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explored in the lesson, you will train the acoustic model with the [CTC loss](http://www.cs.toronto.edu/~graves/icml_2006.pdf) criterion.  Custom loss functions take a bit of hacking in Keras, and so we have implemented the CTC loss function for you, so that you can focus on trying out as many deep learning architectures as possible :).  If you'd like to peek at the implementation details, look at the `add_ctc_loss` function within the `train_utils.py` file in the repository.\n",
    "\n",
    "To train your architecture, you will use the `train_model` function within the `train_utils` module; it has already been imported in one of the above code cells.  The `train_model` function takes three **required** arguments:\n",
    "- `input_to_softmax` - a Keras model instance.\n",
    "- `pickle_path` - the name of the pickle file where the loss history will be saved.\n",
    "- `save_model_path` - the name of the HDF5 file where the model will be saved.\n",
    "\n",
    "If we have already supplied values for `input_to_softmax`, `pickle_path`, and `save_model_path`, please **DO NOT** modify these values. \n",
    "\n",
    "There are several **optional** arguments that allow you to have more control over the training process.  You are welcome to, but not required to, supply your own values for these arguments.\n",
    "- `minibatch_size` - the size of the minibatches that are generated while training the model (default: `20`).\n",
    "- `spectrogram` - Boolean value dictating whether spectrogram (`True`) or MFCC (`False`) features are used for training (default: `True`).\n",
    "- `mfcc_dim` - the size of the feature dimension to use when generating MFCC features (default: `13`).\n",
    "- `optimizer` - the Keras optimizer used to train the model (default: `SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)`).  \n",
    "- `epochs` - the number of epochs to use to train the model (default: `20`).  If you choose to modify this parameter, make sure that it is *at least* 20.\n",
    "- `verbose` - controls the verbosity of the training output in the `model.fit_generator` method (default: `1`).\n",
    "- `sort_by_duration` - Boolean value dictating whether the training and validation sets are sorted by (increasing) duration before the start of the first epoch (default: `False`).\n",
    "\n",
    "The `train_model` function defaults to using spectrogram features; if you choose to use these features, note that the acoustic model in `simple_rnn_model` should have `input_dim=161`.  Otherwise, if you choose to use MFCC features, the acoustic model should have `input_dim=13`.\n",
    "\n",
    "We have chosen to use `GRU` units in the supplied RNN.  If you would like to experiment with `LSTM` or `SimpleRNN` cells, feel free to do so here.  If you change the `GRU` units to `SimpleRNN` cells in `simple_rnn_model`, you may notice that the loss quickly becomes undefined (`nan`) - you are strongly encouraged to check this for yourself!  This is due to the [exploding gradients problem](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/).  We have already implemented [gradient clipping](https://arxiv.org/pdf/1211.5063.pdf) in your optimizer to help you avoid this issue.\n",
    "\n",
    "__IMPORTANT NOTE:__ If you notice that your gradient has exploded in any of the models below, feel free to explore more with gradient clipping (the `clipnorm` argument in your optimizer) or swap out any `SimpleRNN` cells for `LSTM` or `GRU` cells.  You can also try restarting the kernel to restart the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_0, \n",
    "            pickle_path='model_0.pickle', \n",
    "            save_model_path='model_0.h5',\n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model1'></a>\n",
    "### (IMPLEMENTATION) Model 1: RNN + TimeDistributed Dense\n",
    "\n",
    "Read about the [TimeDistributed](https://keras.io/layers/wrappers/) wrapper and the [BatchNormalization](https://keras.io/layers/normalization/) layer in the Keras documentation.  For your next architecture, you will add [batch normalization](https://arxiv.org/pdf/1510.01378.pdf) to the recurrent layer to reduce training times.  The `TimeDistributed` layer will be used to find more complex patterns in the dataset.  The unrolled snapshot of the architecture is depicted below.\n",
    "\n",
    "<img src=\"images/rnn_model.png\" width=\"60%\">\n",
    "\n",
    "The next figure shows an equivalent, rolled depiction of the RNN that shows the (`TimeDistrbuted`) dense and output layers in greater detail.  \n",
    "\n",
    "<img src=\"images/rnn_model_unrolled.png\" width=\"60%\">\n",
    "\n",
    "Use your research to complete the `rnn_model` function within the `sample_models.py` file.  The function should specify an architecture that satisfies the following requirements:\n",
    "- The first layer of the neural network should be an RNN (`SimpleRNN`, `LSTM`, or `GRU`) that takes the time sequence of audio features as input.  We have added `GRU` units for you, but feel free to change `GRU` to `SimpleRNN` or `LSTM`, if you like!\n",
    "- Whereas the architecture in `simple_rnn_model` treated the RNN output as the final layer of the model, you will use the output of your RNN as a hidden layer.  Use `TimeDistributed` to apply a `Dense` layer to each of the time steps in the RNN output.  Ensure that each `Dense` layer has `output_dim` units.\n",
    "\n",
    "Use the code cell below to load your model into the `model_1` variable.  Use a value for `input_dim` that matches your chosen audio features, and feel free to change the values for `units` and `activation` to tweak the behavior of your recurrent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1 = rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                    units=200,\n",
    "                    activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the code cell below to train the neural network you specified in `input_to_softmax`.  After the model has finished training, the model is [saved](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) in the HDF5 file `model_1.h5`.  The loss history is [saved](https://wiki.python.org/moin/UsingPickle) in `model_1.pickle`.  You are welcome to tweak any of the optional parameters while calling the `train_model` function, but this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_1, \n",
    "            pickle_path='model_1.pickle', \n",
    "            save_model_path='model_1.h5',\n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model2'></a>\n",
    "### (IMPLEMENTATION) Model 2: CNN + RNN + TimeDistributed Dense\n",
    "\n",
    "The architecture in `cnn_rnn_model` adds an additional level of complexity, by introducing a [1D convolution layer](https://keras.io/layers/convolutional/#conv1d).  \n",
    "\n",
    "<img src=\"images/cnn_rnn_model.png\" width=\"100%\">\n",
    "\n",
    "This layer incorporates many arguments that can be (optionally) tuned when calling the `cnn_rnn_model` module.  We provide sample starting parameters, which you might find useful if you choose to use spectrogram audio features.  \n",
    "\n",
    "If you instead want to use MFCC features, these arguments will have to be tuned.  Note that the current architecture only supports values of `'same'` or `'valid'` for the `conv_border_mode` argument.\n",
    "\n",
    "When tuning the parameters, be careful not to choose settings that make the convolutional layer overly small.  If the temporal length of the CNN layer is shorter than the length of the transcribed text label, your code will throw an error.\n",
    "\n",
    "Before running the code cell below, you must modify the `cnn_rnn_model` function in `sample_models.py`.  Please add batch normalization to the recurrent layer, and provide the same `TimeDistributed` layer as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filters`: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "\n",
    "`kernel_size`: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "\n",
    "`strides`: An integer or tuple/list of a single integer, specifying the stride length of the convolution. \n",
    "\n",
    "`padding`: One of \"valid\", \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 200)         354400    \n",
      "_________________________________________________________________\n",
      "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "rnn (SimpleRNN)              (None, None, 200)         80200     \n",
      "_________________________________________________________________\n",
      "bn_rnn (BatchNormalization)  (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 442,029\n",
      "Trainable params: 441,229\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/layers/recurrent.py:1031: UserWarning: The `implementation` argument in `SimpleRNN` has been deprecated. Please remove it from your layer call.\n",
      "  warnings.warn('The `implementation` argument '\n"
     ]
    }
   ],
   "source": [
    "model_2 = cnn_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                        filters=200,\n",
    "                        kernel_size=11, \n",
    "                        conv_stride=2,\n",
    "                        conv_border_mode='valid',\n",
    "                        units=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the code cell below to train the neural network you specified in `input_to_softmax`.  After the model has finished training, the model is [saved](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) in the HDF5 file `model_2.h5`.  The loss history is [saved](https://wiki.python.org/moin/UsingPickle) in `model_2.pickle`.  You are welcome to tweak any of the optional parameters while calling the `train_model` function, but this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      " 59/101 [================>.............] - ETA: 6s - loss: 255.7265"
     ]
    }
   ],
   "source": [
    "train_model(input_to_softmax=model_2, \n",
    "            pickle_path='model_2.pickle', \n",
    "            save_model_path='model_2.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model3'></a>\n",
    "### (IMPLEMENTATION) Model 3: Deeper RNN + TimeDistributed Dense\n",
    "\n",
    "Review the code in `rnn_model`, which makes use of a single recurrent layer.  Now, specify an architecture in `deep_rnn_model` that utilizes a variable number `recur_layers` of recurrent layers.  The figure below shows the architecture that should be returned if `recur_layers=2`.  In the figure, the output sequence of the first recurrent layer is used as input for the next recurrent layer.\n",
    "\n",
    "<img src=\"images/deep_rnn_model.png\" width=\"80%\">\n",
    "\n",
    "Feel free to change the supplied values of `units` to whatever you think performs best.  You can change the value of `recur_layers`, as long as your final value is greater than 1. (As a quick check that you have implemented the additional functionality in `deep_rnn_model` correctly, make sure that the architecture that you specify here is identical to `rnn_model` if `recur_layers=1`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = deep_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                         units=200,\n",
    "                         recur_layers=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the code cell below to train the neural network you specified in `input_to_softmax`.  After the model has finished training, the model is [saved](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) in the HDF5 file `model_3.h5`.  The loss history is [saved](https://wiki.python.org/moin/UsingPickle) in `model_3.pickle`.  You are welcome to tweak any of the optional parameters while calling the `train_model` function, but this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_3, \n",
    "            pickle_path='model_3.pickle', \n",
    "            save_model_path='model_3.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model4'></a>\n",
    "### (IMPLEMENTATION) Model 4: Bidirectional RNN + TimeDistributed Dense\n",
    "\n",
    "Read about the [Bidirectional](https://keras.io/layers/wrappers/) wrapper in the Keras documentation.  For your next architecture, you will specify an architecture that uses a single bidirectional RNN layer, before a (`TimeDistributed`) dense layer.  The added value of a bidirectional RNN is described well in [this paper](http://www.cs.toronto.edu/~hinton/absps/DRNN_speech.pdf).\n",
    "> One shortcoming of conventional RNNs is that they are only able to make use of previous context. In speech recognition, where whole utterances are transcribed at once, there is no reason not to exploit future context as well.  Bidirectional RNNs (BRNNs) do this by processing the data in both directions with two separate hidden layers which are then fed forwards to the same output layer.\n",
    "\n",
    "<img src=\"images/bidirectional_rnn_model.png\" width=\"80%\">\n",
    "\n",
    "Before running the code cell below, you must complete the `bidirectional_rnn_model` function in `sample_models.py`.  Feel free to use `SimpleRNN`, `LSTM`, or `GRU` units.  When specifying the `Bidirectional` wrapper, use `merge_mode='concat'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = bidirectional_rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                                  units=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the code cell below to train the neural network you specified in `input_to_softmax`.  After the model has finished training, the model is [saved](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) in the HDF5 file `model_4.h5`.  The loss history is [saved](https://wiki.python.org/moin/UsingPickle) in `model_4.pickle`.  You are welcome to tweak any of the optional parameters while calling the `train_model` function, but this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(input_to_softmax=model_4, \n",
    "            pickle_path='model_4.pickle', \n",
    "            save_model_path='model_4.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model5'></a>\n",
    "### (OPTIONAL IMPLEMENTATION) Models 5+\n",
    "\n",
    "If you would like to try out more architectures than the ones above, please use the code cell below.  Please continue to follow the same convention for saving the models; for the $i$-th sample model, please save the loss at **`model_i.pickle`** and saving the trained model at **`model_i.h5`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Try out some more models!\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='compare'></a>\n",
    "### Compare the Models\n",
    "\n",
    "Execute the code cell below to evaluate the performance of the drafted deep learning models.  The training and validation loss are plotted for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set_style(style='white')\n",
    "\n",
    "# obtain the paths for the saved model history\n",
    "all_pickles = sorted(glob(\"results/*.pickle\"))\n",
    "all_pickles = ['results/model_1.pickle',\n",
    " 'results/model_2.pickle',\n",
    " 'results/model_3.pickle',\n",
    " 'results/model_4.pickle']\n",
    "# extract the name of each model\n",
    "model_names = [item[8:-7] for item in all_pickles]\n",
    "# extract the loss history for each model\n",
    "valid_loss = [pickle.load( open( i, \"rb\" ) )['val_loss'] for i in all_pickles]\n",
    "train_loss = [pickle.load( open( i, \"rb\" ) )['loss'] for i in all_pickles]\n",
    "# save the number of epochs used to train each model\n",
    "num_epochs = [len(valid_loss[i]) for i in range(len(valid_loss))]\n",
    "\n",
    "fig = plt.figure(figsize=(16,5))\n",
    "\n",
    "# plot the training loss vs. epoch for each model\n",
    "ax1 = fig.add_subplot(121)\n",
    "for i in range(len(all_pickles)):\n",
    "    ax1.plot(np.linspace(1, num_epochs[i], num_epochs[i]), \n",
    "            train_loss[i], label=model_names[i])\n",
    "# clean up the plot\n",
    "ax1.legend()  \n",
    "ax1.set_xlim([1, max(num_epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "# plot the validation loss vs. epoch for each model\n",
    "ax2 = fig.add_subplot(122)\n",
    "for i in range(len(all_pickles)):\n",
    "    ax2.plot(np.linspace(1, num_epochs[i], num_epochs[i]), \n",
    "            valid_loss[i], label=model_names[i])\n",
    "# clean up the plot\n",
    "ax2.legend()  \n",
    "ax2.set_xlim([1, max(num_epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1:__ Use the plot above to analyze the performance of each of the attempted architectures.  Which performs best?  Provide an explanation regarding why you think some models perform better than others. \n",
    "\n",
    "__Answer:__\n",
    "\n",
    "Model 0 is excluded from the graphs because it did far worse than others and would make it harder to see how other competing models do relative to each other. Among all the other models they all have similar performance for practical purposes. Model 1 has a strange behavior as its training loss seem to start to increase over time. Across validation losses we can see that 10-12 epoches is enough for training and beyond that we may start to overfit to the training data. \n",
    "I particularly like model 2 because it trained much faster than other models and it has a comparative performance against others. I should admit the keras documentation around Conv1D is very confusing. The best performing model is the deep RNN model. I would like to combine CNN + Deeper RNN for my final model based on the results above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final'></a>\n",
    "### (IMPLEMENTATION) Final Model\n",
    "\n",
    "Now that you've tried out many sample models, use what you've learned to draft your own architecture!  While your final acoustic model should not be identical to any of the architectures explored above, you are welcome to merely combine the explored layers above into a deeper architecture.  It is **NOT** necessary to include new layer types that were not explored in the notebook.\n",
    "\n",
    "However, if you would like some ideas for even more layer types, check out these ideas for some additional, optional extensions to your model:\n",
    "\n",
    "- If you notice your model is overfitting to the training dataset, consider adding **dropout**!  To add dropout to [recurrent layers](https://faroit.github.io/keras-docs/1.0.2/layers/recurrent/), pay special attention to the `dropout_W` and `dropout_U` arguments.  This [paper](http://arxiv.org/abs/1512.05287) may also provide some interesting theoretical background.\n",
    "- If you choose to include a convolutional layer in your model, you may get better results by working with **dilated convolutions**.  If you choose to use dilated convolutions, make sure that you are able to accurately calculate the length of the acoustic model's output in the `model.output_length` lambda function.  You can read more about dilated convolutions in Google's [WaveNet paper](https://arxiv.org/abs/1609.03499).  For an example of a speech-to-text system that makes use of dilated convolutions, check out this GitHub [repository](https://github.com/buriburisuri/speech-to-text-wavenet).  You can work with dilated convolutions [in Keras](https://keras.io/layers/convolutional/) by paying special attention to the `padding` argument when you specify a convolutional layer.\n",
    "- If your model makes use of convolutional layers, why not also experiment with adding **max pooling**?  Check out [this paper](https://arxiv.org/pdf/1701.02720.pdf) for example architecture that makes use of max pooling in an acoustic model.\n",
    "- So far, you have experimented with a single bidirectional RNN layer.  Consider stacking the bidirectional layers, to produce a [deep bidirectional RNN](https://www.cs.toronto.edu/~graves/asru_2013.pdf)!\n",
    "\n",
    "All models that you specify in this repository should have `output_length` defined as an attribute.  This attribute is a lambda function that maps the (temporal) length of the input acoustic features to the (temporal) length of the output softmax layer.  This function is used in the computation of CTC loss; to see this, look at the `add_ctc_loss` function in `train_utils.py`.  To see where the `output_length` attribute is defined for the models in the code, take a look at the `sample_models.py` file.  You will notice this line of code within most models:\n",
    "```\n",
    "model.output_length = lambda x: x\n",
    "```\n",
    "The acoustic model that incorporates a convolutional layer (`cnn_rnn_model`) has a line that is a bit different:\n",
    "```\n",
    "model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "```\n",
    "\n",
    "In the case of models that use purely recurrent layers, the lambda function is the identity function, as the recurrent layers do not modify the (temporal) length of their input tensors.  However, convolutional layers are more complicated and require a specialized function (`cnn_output_length` in `sample_models.py`) to determine the temporal length of their output.\n",
    "\n",
    "You will have to add the `output_length` attribute to your final model before running the code cell below.  Feel free to use the `cnn_output_length` function, if it suits your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 512)         907264    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "bn_conv_1d (BatchNormalizati (None, None, 512)         2048      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, None, 1024)        3148800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, None, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "rnn_dropout0 (Dropout)       (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, None, 1024)        4721664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, None, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "rnn_dropout1 (Dropout)       (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, None, 1024)        4721664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, None, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "rnn_dropout2 (Dropout)       (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, None, 29)          29725     \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 13,543,453\n",
      "Trainable params: 13,536,285\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# specify the model\n",
    "model_end = final_model(input_dim=161,units=512,recur_layers=3) \n",
    "\n",
    "# model_end = model_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please execute the code cell below to train the neural network you specified in `input_to_softmax`.  After the model has finished training, the model is [saved](https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model) in the HDF5 file `model_end.h5`.  The loss history is [saved](https://wiki.python.org/moin/UsingPickle) in `model_end.pickle`.  You are welcome to tweak any of the optional parameters while calling the `train_model` function, but this is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4249: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4229: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/20\n",
      " 71/101 [====================>.........] - ETA: 10s - loss: 666.4599"
     ]
    }
   ],
   "source": [
    "train_model(input_to_softmax=model_end, \n",
    "            pickle_path='model_end.pickle', \n",
    "            save_model_path='model_end.h5', \n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ Describe your final model architecture and your reasoning at each step.  \n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## STEP 3: Obtain Predictions\n",
    "\n",
    "We have written a function for you to decode the predictions of your acoustic model.  To use the function, please execute the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from utils import int_sequence_to_text\n",
    "from IPython.display import Audio\n",
    "\n",
    "def get_predictions(index, partition, input_to_softmax, model_path):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        index (int): The example you would like to visualize\n",
    "        partition (str): One of 'train' or 'validation'\n",
    "        input_to_softmax (Model): The acoustic model\n",
    "        model_path (str): Path to saved acoustic model's weights\n",
    "    \"\"\"\n",
    "    # load the train and test data\n",
    "    data_gen = AudioGenerator()\n",
    "    data_gen.load_train_data()\n",
    "    data_gen.load_validation_data()\n",
    "    \n",
    "    # obtain the true transcription and the audio features \n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    # obtain and decode the acoustic model's predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    Audio(audio_path)\n",
    "    print('True transcription:\\n' + '\\n' + transcr)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code cell below to obtain the transcription predicted by your final model for the first example in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 200)         354400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "0 (GRU)                      (None, None, 200)         240600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "1 (GRU)                      (None, None, 200)         240600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 843,829\n",
      "Trainable params: 842,629\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4303: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "her father is a most remarkable person to say the least\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription:\n",
      "\n",
      "br rs ws r rpr b prsin s   s\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index=0, \n",
    "                partition='train',\n",
    "                input_to_softmax=final_model(input_dim=161,units=200,recur_layers=2)  , \n",
    "                model_path='./results/model_end.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the model's prediction for the first example in the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, None, 200)         354400    \n",
      "_________________________________________________________________\n",
      "average_pooling1d_3 (Average (None, None, 200)         0         \n",
      "_________________________________________________________________\n",
      "bn_conv_1d (BatchNormalizati (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "0 (GRU)                      (None, None, 200)         240600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "1 (GRU)                      (None, None, 200)         240600    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 843,829\n",
      "Trainable params: 842,629\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "the bogus legislature numbered thirty six members\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription:\n",
      "\n",
      "tho b wsr sd tr r r  rysirs n rs\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index=0, \n",
    "                partition='validation',\n",
    "                input_to_softmax=final_model(input_dim=161,units=200,recur_layers=2) , \n",
    "                model_path='./results/model_end.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One standard way to improve the results of the decoder is to incorporate a language model.  We won't pursue this in the notebook, but you are welcome to do so as an _optional extension_. \n",
    "\n",
    "If you are interested in creating models that provide improved transcriptions, you are encouraged to download [more data](http://www.openslr.org/12/) and train bigger, deeper models.  But beware - the model will likely take a long while to train.  For instance, training this [state-of-the-art](https://arxiv.org/pdf/1512.02595v1.pdf) model would take 3-6 weeks on a single GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A lesson on 1 and 2D Conv in Keras\n",
    "The keras documentation sucks big time so i including some examples here.\n",
    "it seems the standard input in keras is of the following shape\n",
    "`(batch_size, words or steps, channels or input dimension)`\n",
    "for example in case of NLP, a sentence can be input to the model\n",
    "\n",
    "`steps` = number of words in the sentence (usually padded to a max length)\n",
    "`input_dimension` = size of word embedding vectors for example 128\n",
    "\n",
    "in an image application (not entirely sure) \n",
    "\n",
    "`steps` = i think this might be number of pixels in an image\n",
    "`input_dimension` = if it is an RGB image, the three color channels \n",
    "\n",
    "Assume an input/sentence of `(batch = 1, steps = 4 words, input_dimension = 5 integers embeddings per word)` if you apply :\n",
    "\n",
    "`Conv1D(filters=3, kernel_size=1, kernel_initializer='ones')`\n",
    "\n",
    "it will create a `(1, 5, 3)` tensor. Notice in conv1D there is no mention of 5 (input dimension). So when you say i want 3 filters of kernel size 1, it will create 3 filter of size 5 (channels) for you. so the Conv1D has 3x5 + 3 biases = 18 parameters. it will multiply (dot product) the first of these `(1x5)` filters with the first row/step/word-embedding of your input. This will generate one number. Then multiplies the other 2 filters with the first row/step/word-embedding and this will create 2 other numbers to build the first row of your ouput. Thus, The first row of output then has 3 elements. The same process if applied to second and third row/step/word-embedding in the input sentence. The final result is 1x4x3. \n",
    "\n",
    "`Conv1D(filters=3, kernel_size=2, kernel_initializer='ones')`\n",
    "\n",
    "will create a tensor of shape `(2, 5, 3)`. This is very different from intuition. Intuitively you think if you increase kernal you are increasing the size of your filter by 1. In contrary this will add another dimension to your Conv tensor. \n",
    "Now, it seems from example below `(1, 5, 3)`  is applied to first row of input/senetnce generating 1 number, and the second `(1, 5, 3)` is applied to the second row of input/sentence generating the second number. The first and second number are added to gether to create the first element of the output. You can see the filter is now big enough to expand 2 words. So after repeating this the output has a shape of 1x3x3. if you use the same padding the input shape is changed to \n",
    "`(batch = 1, steps = 5 words, input_dimension = 5 integers embeddings per word)` where the last added word has only 0 embeddings. \n",
    "\n",
    "notice in the context of voice recognition as we increase the kernel length the output of 1D convultion becomes shorter and shorter compared to the input sequence length. it is like saying i input 20 second worth of recording but only get 15 seconds worth of output out of 1dConv with a larger kernel size (kernerl size of 6). This is when you use a `valid` padding, other wise you should get the same length with `same` padding\n",
    "\n",
    "\n",
    "as shown max pooling decreases the size of steps by the pool_size=2 or more precisely steps from the previous layer // pool_size if the padding is `valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, Conv2D\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "for kernel_size in [1,2]:\n",
    "    # create an input with 4 steps and 5 channels/input_dim\n",
    "    channels = 5 # each word is embedded by 5 numbers \n",
    "    steps = 4  # there are 4 words or 4 seconds worth of recording \n",
    "    filters = 3 \n",
    "    # output would be of shape (batch_size, steps - kernel_size + 1, filters)\n",
    "    # each filter has a shape of (1 x 5) , where 5 is the size of embedding\n",
    "    val = np.array([list(range(i * channels, (i + 1) * channels)) for i in range(1, steps + 1)])\n",
    "    val = np.expand_dims(val, axis=0)\n",
    "    x = K.variable(value=val)\n",
    "\n",
    "    # 1D convolution. Initialize the kernels to ones so that it's easier to compute the result by hand\n",
    "\n",
    "    conv1d = Conv1D(filters=3, kernel_size=kernel_size, kernel_initializer='ones', padding='same')\n",
    "    conv1d_imp = conv1d(x)\n",
    "\n",
    "    # 2D convolution that replicates the 1D one\n",
    "\n",
    "    # need to add a dimension to your input since conv2d expects 4D inputs. I add it at axis 4 since my keras is setup with `channel_last`\n",
    "    val1 = np.expand_dims(val, axis=3)\n",
    "    x1 = K.variable(value=val1)\n",
    "\n",
    "    conv2d = Conv2D(filters=3, kernel_size=(kernel_size, 5), kernel_initializer='ones')(x1)\n",
    "\n",
    "    # evaluate and print the outputs\n",
    "    print(\"result with kernel size: \", kernel_size)\n",
    "    print(\"shape of input \\n\", val.shape)\n",
    "    print(\"input values \\n\", val)\n",
    "\n",
    "    print(\"shape of the 1D conv \\n\", conv1d.get_weights()[0].shape)\n",
    "    print(\"weights of 1D conv \\n\", conv1d.get_weights()[0])\n",
    "    print(\"biases of 1D conv \\n\", conv1d.get_weights()[1])\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print(\"result with kernel size: \", kernel_size)\n",
    "    print(\"1DCONV\")\n",
    "    print(\"shape of output of the 1D conv process \\n\", K.eval(conv1d_imp).shape)\n",
    "    print(\"1DConv output \\n\", K.eval(conv1d_imp))\n",
    "    print(\"--------------\")\n",
    "\n",
    "\n",
    "    print(\"--------------\")\n",
    "    print(\"2DCONV\")\n",
    "    print(\"2DConv output \\n\", K.eval(conv2d))\n",
    "    print(\"--------------\")\n",
    "\n",
    "    if kernel_size == 1:\n",
    "        print(\"--------------\")\n",
    "        print(\"result with kernel size: \", kernel_size)\n",
    "        print(\"changing the weights of the 1Dconv manually\") \n",
    "        conv1d.set_weights([np.array( [[[1., 2., 1.],[1., 2., 1.],[1., 2., 1.],[1., 2., 1.],[1., 2., 1.]]]),np.array([0, 0, 0])])\n",
    "        conv1d_imp = conv1d(x)\n",
    "        print(\"weights of 1D conv \\n\", conv1d.get_weights()[0])\n",
    "        print(\"biases of 1D conv \\n\", conv1d.get_weights()[1])\n",
    "        print(\"1DCONV\")\n",
    "        print(\"shape of output of the 1D conv process \\n\", K.eval(conv1d_imp).shape)\n",
    "        print(\"1DConv output \\n\", K.eval(conv1d_imp))\n",
    "        print(\"--------------\")\n",
    "        \n",
    "        \n",
    "    if kernel_size == 2:\n",
    "        print(\"--------------\")\n",
    "        print(\"result with kernel size: \", kernel_size)\n",
    "        print(\"changing the weights of the 1Dconv manually\") \n",
    "        initial_value = np.ones(shape=(2,5,3))\n",
    "        initial_value[1,:,1] = 2\n",
    "        conv1d.set_weights([initial_value,np.array([0, 0, 0])])\n",
    "        conv1d_imp = conv1d(x)\n",
    "        print(\"weights of 1D conv \\n\", conv1d.get_weights()[0])\n",
    "        print(\"biases of 1D conv \\n\", conv1d.get_weights()[1])\n",
    "        print(\"1DCONV\")\n",
    "        print(\"shape of output of the 1D conv process \\n\", K.eval(conv1d_imp).shape)\n",
    "        print(\"1DConv output \\n\", K.eval(conv1d_imp))\n",
    "        print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of first words embedding elements\")\n",
    "5 + 6+ 7+ 8 + 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of second words embedding elements\")\n",
    "10 + 11+ 12+ 13 + 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of third words embedding elements\")\n",
    "15 + 16+ 17+ 18 + 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of forth words embedding elements\")\n",
    "20 + 21 + 22 + 23 + 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result with kernel size:  1\n",
      "shape of input \n",
      " (1, 4, 5)\n",
      "input values \n",
      " [[[ 5  6  7  8  9]\n",
      "  [10 11 12 13 14]\n",
      "  [15 16 17 18 19]\n",
      "  [20 21 22 23 24]]]\n",
      "shape of the 1D conv \n",
      " (1, 5, 3)\n",
      "weights of 1D conv \n",
      " [[[1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "biases of 1D conv \n",
      " [0. 0. 0.]\n",
      "--------------\n",
      "result with kernel size:  1\n",
      "1DCONV\n",
      "shape of output of the 1D conv process \n",
      " (1, 4, 3)\n",
      "1DConv output \n",
      " [[[ 35.  35.  35.]\n",
      "  [ 60.  60.  60.]\n",
      "  [ 85.  85.  85.]\n",
      "  [110. 110. 110.]]]\n",
      "max pooling output \n",
      " [[[85. 85. 85.]]]\n",
      "max pooling output shape \n",
      " (1, 1, 3)\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, Conv2D, MaxPool1D, AveragePooling1D\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create an input with 4 steps and 5 channels/input_dim\n",
    "channels = 5\n",
    "steps = 4\n",
    "filters = 3\n",
    "kernel_size = 1\n",
    "val = np.array([list(range(i * channels, (i + 1) * channels)) for i in range(1, steps + 1)])\n",
    "val = np.expand_dims(val, axis=0)\n",
    "x = K.variable(value=val)\n",
    "\n",
    "# 1D convolution. Initialize the kernels to ones so that it's easier to compute the result by hand\n",
    "\n",
    "conv1d = Conv1D(filters=3, kernel_size=kernel_size, kernel_initializer='ones', padding='valid')\n",
    "conv1d_imp = conv1d(x)\n",
    "pooled_layer = MaxPool1D(pool_size=3, strides=None, padding='valid', data_format='channels_last')(conv1d_imp)\n",
    "\n",
    "# evaluate and print the outputs\n",
    "print(\"result with kernel size: \", kernel_size)\n",
    "print(\"shape of input \\n\", val.shape)\n",
    "print(\"input values \\n\", val)\n",
    "\n",
    "print(\"shape of the 1D conv \\n\", conv1d.get_weights()[0].shape)\n",
    "print(\"weights of 1D conv \\n\", conv1d.get_weights()[0])\n",
    "print(\"biases of 1D conv \\n\", conv1d.get_weights()[1])\n",
    "\n",
    "print(\"--------------\")\n",
    "print(\"result with kernel size: \", kernel_size)\n",
    "print(\"1DCONV\")\n",
    "print(\"shape of output of the 1D conv process \\n\", K.eval(conv1d_imp).shape)\n",
    "print(\"1DConv output \\n\", K.eval(conv1d_imp))\n",
    "print(\"max pooling output \\n\", K.eval(pooled_layer))\n",
    "print(\"max pooling output shape \\n\", K.eval(pooled_layer).shape)\n",
    "print(\"--------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Input API\n",
    "\n",
    "Arguments to the Input: shape: defines the shape of a single sample, with variable batch size \n",
    "\n",
    "`#inp will be a tensor with shape (?, 28, 28)`\n",
    "\n",
    "`inp = Input((28,28))`\n",
    "\n",
    "in example below we do not know the steps/sequence/number-of-words/length-of-voice-recording in the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "Input(name='the_input', shape=(None, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pemfir/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-54127f003301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n\u001b[1;32m      2\u001b[0m     TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM, Dropout)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_gpuenv/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
     ]
    }
   ],
   "source": [
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM, Dropout)\n",
    "A = LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow_gpuenv",
   "language": "python",
   "name": "tensorflow_gpuenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
